{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5 - Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Features and Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Fight_Name', 'Date', 'Location', 'R', 'B', 'Division', 'Format', 'Referee', 'R_Name', 'R_Age', 'R_Height', 'R_Weight', 'R_Reach', 'R_Stance', 'R_Birthday', 'R_SSLPM', 'R_SAC', 'R_SSAPM', 'R_SD', 'R_TAV', 'R_TAC', 'R_TD','R_SAV', 'B_Name', 'B_Age', 'B_Height', 'B_Weight', 'B_Reach', 'B_Stance', 'B_Birthday', 'B_SSLPM', 'B_SAC', 'B_SSAPM', 'B_SD', 'B_TAV', 'B_TAC', 'B_TD', 'B_SAV']]\n",
    "Y = df['Winner']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train = 60%, Validation = 20%, Test = 20%\n",
    "X_TV, X_Test, Y_TV, Y_Test = train_test_split(X, Y, test_size = 0.2, random_state = 3)\n",
    "X_Train, X_Validation, Y_Train, Y_Validation = train_test_split(X_TV, Y_TV, test_size = 0.25, random_state = 3)\n",
    "\n",
    "print(f'X Shape: {X.shape} | X Type: {type(X)}')\n",
    "print(f'Y Shape: {Y.shape} | Y Type: {type(Y)}')\n",
    "print(f'X_Train Shape: {X_Train.shape} | X_Train Type: {type(X_Train)}')\n",
    "print(f'X_Validation Shape: {X_Validation.shape} | X_Validation Type: {type(X_Validation)}')\n",
    "print(f'X_Test Shape: {X_Test.shape} | X_Test Type: {type(X_Test)}')\n",
    "print(f'Y_Train Shape: {Y_Train.shape} | Y_Train Type: {type(Y_Train)}')\n",
    "print(f'Y_Validation Shape: {Y_Validation.shape} | Y_Validation Type: {type(Y_Validation)}')\n",
    "print(f'Y_Test Shape: {Y_Test.shape} | Y_Test Type: {type(Y_Test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = X_Train.reset_index(drop = True)\n",
    "X_Validation = X_Validation.reset_index(drop = True)\n",
    "X_Test = X_Test.reset_index(drop = True)\n",
    "Y_Train = Y_Train.reset_index(drop = True)\n",
    "Y_Validation = Y_Validation.reset_index(drop = True)\n",
    "Y_Test = Y_Test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Numeric Columns\n",
    "Numeric_Columns = X_Train.select_dtypes(include = ['float64', 'int64']).columns\n",
    "print('Numeric Columns:', Numeric_Columns)\n",
    "print('# of Numeric Columns:', len(Numeric_Columns))\n",
    "print('------------------------------------')\n",
    "\n",
    "# Selecting Object Columns\n",
    "Object_Columns = X_Train.select_dtypes(include = ['object', 'category', 'datetime64']).columns\n",
    "print('Object Columns:', Object_Columns)\n",
    "print('# of Object Columns:', len(Object_Columns))\n",
    "print('------------------------------------')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling X\n",
    "X_Scaler = StandardScaler()\n",
    "\n",
    "# Fit Scaler to X_Train\n",
    "X_Scaler.fit(X_Train[Numeric_Columns]) \n",
    "\n",
    "# Scale Numeric Columns\n",
    "X_Train_Numeric_STD = X_Scaler.transform(X_Train[Numeric_Columns]) \n",
    "X_Validation_Numeric_STD = X_Scaler.transform(X_Validation[Numeric_Columns])\n",
    "X_Test_Numeric_STD = X_Scaler.transform(X_Test[Numeric_Columns])\n",
    "\n",
    "# Scaler automatically converts pd DataFrame to 2D np Array, so we need to convert it back to pd DataFrame\n",
    "X_Train_Numeric_STD = pd.DataFrame(X_Train_Numeric_STD, columns = Numeric_Columns)\n",
    "X_Validation_Numeric_STD = pd.DataFrame(X_Validation_Numeric_STD, columns = Numeric_Columns)\n",
    "X_Test_Numeric_STD = pd.DataFrame(X_Test_Numeric_STD, columns = Numeric_Columns)\n",
    "\n",
    "# Checking Shapes and Indices before Concatenation\n",
    "print(\"Shapes Before Concatenation:\")\n",
    "print(X_Train_Numeric_STD.shape)\n",
    "print(X_Train[Object_Columns].shape)\n",
    "print(\"Indices Before Concatenation:\")\n",
    "print(X_Train_Numeric_STD.index)\n",
    "print(X_Train[Object_Columns].index)\n",
    "print('------------------------------------')\n",
    "\n",
    "# Concatenate Numeric and Object Columns\n",
    "X_Train = pd.concat([X_Train_Numeric_STD, X_Train[Object_Columns]], axis = 1)\n",
    "X_Validation = pd.concat([X_Validation_Numeric_STD, X_Validation[Object_Columns]], axis = 1)\n",
    "X_Test = pd.concat([X_Test_Numeric_STD, X_Test[Object_Columns]], axis = 1)\n",
    "\n",
    "# Checking Shapes and Indices after Concatenation\n",
    "print(\"Shapes After Concatenation:\")\n",
    "print(X_Train.shape)\n",
    "print(\"Indices After Concatenation:\")\n",
    "print(X_Train.index)\n",
    "\n",
    "print(Y_Train.index)\n",
    "\n",
    "# Re-Ordering Columns\n",
    "order = ['Fight_Name', 'Date', 'Location', 'R', 'B', 'Division', 'Format', 'Referee',  \n",
    "        'R_Name', 'R_Age', 'R_Height', 'R_Weight', 'R_Reach', 'R_Stance', 'R_Birthday', 'R_SSLPM', 'R_SAC', 'R_SSAPM', 'R_SD', 'R_TAV', 'R_TAC', 'R_TD','R_SAV',\n",
    "        'B_Name', 'B_Age', 'B_Height', 'B_Weight', 'B_Reach', 'B_Stance', 'B_Birthday', 'B_SSLPM', 'B_SAC', 'B_SSAPM', 'B_SD', 'B_TAV', 'B_TAC', 'B_TD', 'B_SAV']\n",
    "\n",
    "X_Train = X_Train.reindex(columns = order)\n",
    "X_Validation = X_Validation.reindex(columns = order)\n",
    "X_Test = X_Test.reindex(columns = order)\n",
    "\n",
    "print('************************************')\n",
    "print('Scaled X')\n",
    "display(X_Train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = X_Train.reset_index(drop = True)\n",
    "X_Validation = X_Validation.reset_index(drop = True)\n",
    "X_Test = X_Test.reset_index(drop = True)\n",
    "Y_Train = Y_Train.reset_index(drop = True)\n",
    "Y_Validation = Y_Validation.reset_index(drop = True)\n",
    "Y_Test = Y_Test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Numeric Columns\n",
    "Numeric_Columns = X_Train.select_dtypes(include = ['float64', 'int64']).columns\n",
    "print('Numeric Columns:', Numeric_Columns)\n",
    "print('# of Numeric Columns:', len(Numeric_Columns))\n",
    "print('------------------------------------')\n",
    "\n",
    "# Selecting Object Columns\n",
    "Object_Columns = X_Train.select_dtypes(include = ['object', 'category', 'datetime64']).columns\n",
    "print('Object Columns:', Object_Columns)\n",
    "print('# of Object Columns:', len(Object_Columns))\n",
    "print('------------------------------------')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling X\n",
    "X_Scaler = StandardScaler()\n",
    "\n",
    "# Fit Scaler to X_Train\n",
    "X_Scaler.fit(X_Train[Numeric_Columns]) \n",
    "\n",
    "# Scale Numeric Columns\n",
    "X_Train_Numeric_STD = X_Scaler.transform(X_Train[Numeric_Columns]) \n",
    "X_Validation_Numeric_STD = X_Scaler.transform(X_Validation[Numeric_Columns])\n",
    "X_Test_Numeric_STD = X_Scaler.transform(X_Test[Numeric_Columns])\n",
    "\n",
    "# Scaler automatically converts pd DataFrame to 2D np Array, so we need to convert it back to pd DataFrame\n",
    "X_Train_Numeric_STD = pd.DataFrame(X_Train_Numeric_STD, columns = Numeric_Columns)\n",
    "X_Validation_Numeric_STD = pd.DataFrame(X_Validation_Numeric_STD, columns = Numeric_Columns)\n",
    "X_Test_Numeric_STD = pd.DataFrame(X_Test_Numeric_STD, columns = Numeric_Columns)\n",
    "\n",
    "# Checking Shapes and Indices before Concatenation\n",
    "print(\"Shapes Before Concatenation:\")\n",
    "print(X_Train_Numeric_STD.shape)\n",
    "print(X_Train[Object_Columns].shape)\n",
    "print(\"Indices Before Concatenation:\")\n",
    "print(X_Train_Numeric_STD.index)\n",
    "print(X_Train[Object_Columns].index)\n",
    "print('------------------------------------')\n",
    "\n",
    "# Concatenate Numeric and Object Columns\n",
    "X_Train = pd.concat([X_Train_Numeric_STD, X_Train[Object_Columns]], axis = 1)\n",
    "X_Validation = pd.concat([X_Validation_Numeric_STD, X_Validation[Object_Columns]], axis = 1)\n",
    "X_Test = pd.concat([X_Test_Numeric_STD, X_Test[Object_Columns]], axis = 1)\n",
    "\n",
    "# Checking Shapes and Indices after Concatenation\n",
    "print(\"Shapes After Concatenation:\")\n",
    "print(X_Train.shape)\n",
    "print(\"Indices After Concatenation:\")\n",
    "print(X_Train.index)\n",
    "\n",
    "print(Y_Train.index)\n",
    "\n",
    "# Re-Ordering Columns\n",
    "order = ['Fight_Name', 'Date', 'Location', 'R', 'B', 'Division', 'Format', 'Referee',  \n",
    "        'R_Name', 'R_Age', 'R_Height', 'R_Weight', 'R_Reach', 'R_Stance', 'R_Birthday', 'R_SSLPM', 'R_SAC', 'R_SSAPM', 'R_SD', 'R_TAV', 'R_TAC', 'R_TD','R_SAV',\n",
    "        'B_Name', 'B_Age', 'B_Height', 'B_Weight', 'B_Reach', 'B_Stance', 'B_Birthday', 'B_SSLPM', 'B_SAC', 'B_SSAPM', 'B_SD', 'B_TAV', 'B_TAC', 'B_TD', 'B_SAV']\n",
    "\n",
    "X_Train = X_Train.reindex(columns = order)\n",
    "X_Validation = X_Validation.reindex(columns = order)\n",
    "X_Test = X_Test.reindex(columns = order)\n",
    "\n",
    "print('************************************')\n",
    "print('Scaled X')\n",
    "display(X_Train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority Class\n",
    "print(f'Majority Class: {Y_Train.value_counts().idxmax()} (Red)')\n",
    "\n",
    "print('------------------------------------')\n",
    "# Accuracy\n",
    "print(f\"Training Accuracy: {Y_Train.sum() / Y_Train.count()}\")\n",
    "print(f\"Test Accuracy: {Y_Test.sum() / Y_Test.count()}\")\n",
    "print('------------------------------------')\n",
    "\n",
    "# Loss\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Training Loss\n",
    "# Probabilities\n",
    "Train_R_Predicted_Probability = Y_Train.sum() / Y_Train.count() # Accuracy\n",
    "Train_B_Predicted_Probability = 1 - Train_R_Predicted_Probability\n",
    "# Reshape\n",
    "Train_RPP_RS = np.full(Y_Train.shape, Train_R_Predicted_Probability)\n",
    "Train_BPP_RS = np.full(Y_Train.shape, Train_B_Predicted_Probability)\n",
    "# Concatenate\n",
    "Train_Probabilities = np.column_stack((Train_BPP_RS, Train_RPP_RS))\n",
    "print(f'Training Loss: {log_loss(Y_Train, Train_Probabilities)}')\n",
    "\n",
    "# Test Loss\n",
    "# Probabilities\n",
    "Test_R_Predicted_Probability = Y_Test.sum() / Y_Test.count() # Accuracy\n",
    "Test_B_Predicted_Probability = 1 - Test_R_Predicted_Probability\n",
    "# Reshape\n",
    "Test_RPP_RS = np.full(Y_Test.shape, Test_R_Predicted_Probability)\n",
    "Test_BPP_RS = np.full(Y_Test.shape, Test_B_Predicted_Probability)\n",
    "# Concatenate\n",
    "Test_Probabilities = np.column_stack((Test_BPP_RS, Test_RPP_RS))\n",
    "print(f'Test Loss: {log_loss(Y_Test, Test_Probabilities)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model 1**: Height, Weight, Reach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_M1 = X_Train.copy()\n",
    "Y_Train_M1 = Y_Train.copy()\n",
    "\n",
    "X_Validation_M1 = X_Validation.copy()\n",
    "Y_Validation_M1 = Y_Validation.copy()\n",
    "\n",
    "X_Test_M1 = X_Test.copy()\n",
    "Y_Test_M1 = Y_Test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Features and Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_M1 = X_Train_M1[['R_Height', 'R_Weight', 'R_Reach', 'B_Height', 'B_Weight', 'B_Reach']]\n",
    "X_Validation_M1 = X_Validation_M1[['R_Height', 'R_Weight', 'R_Reach', 'B_Height', 'B_Weight', 'B_Reach']]\n",
    "X_Test_M1 = X_Test_M1[['R_Height', 'R_Weight', 'R_Reach', 'B_Height', 'B_Weight', 'B_Reach']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "print('TRAIN')\n",
    "print(f'Before Drop: {X_Train_M1.shape}')\n",
    "# Identify Missing Indices\n",
    "X_Train_M1_Missing_Indices = X_Train_M1.index[X_Train_M1.isnull().any(axis = 1)]\n",
    "# Drop Missing Indices\n",
    "X_Train_M1 = X_Train_M1.drop(X_Train_M1_Missing_Indices)\n",
    "Y_Train_M1 = Y_Train_M1.drop(X_Train_M1_Missing_Indices)\n",
    "# Reset Index\n",
    "X_Train_M1 = X_Train_M1.reset_index(drop = True)\n",
    "Y_Train_M1 = Y_Train_M1.reset_index(drop = True)\n",
    "print(f'After Drop: {X_Train_M1.shape}')\n",
    "print('------------------------------------')\n",
    "\n",
    "# VALIDATION\n",
    "print('VALIDATION')\n",
    "print(f'Before Drop: {X_Validation_M1.shape}')\n",
    "# Identify Missing Indices\n",
    "X_Validation_M1_Missing_Indices = X_Validation_M1.index[X_Validation_M1.isnull().any(axis = 1)]\n",
    "# Drop Missing Indices\n",
    "X_Validation_M1 = X_Validation_M1.drop(X_Validation_M1_Missing_Indices)\n",
    "Y_Validation_M1 = Y_Validation_M1.drop(X_Validation_M1_Missing_Indices)\n",
    "# Reset Index\n",
    "X_Validation_M1 = X_Validation_M1.reset_index(drop = True)\n",
    "Y_Validation_M1 = Y_Validation_M1.reset_index(drop = True)\n",
    "print(f'After Drop: {X_Validation_M1.shape}')\n",
    "print('------------------------------------')\n",
    "\n",
    "# TEST\n",
    "print('TEST')\n",
    "print(f'Before Drop: {X_Test_M1.shape}')\n",
    "# Identify Missing Indices\n",
    "X_Test_M1_Missing_Indices = X_Test_M1.index[X_Test_M1.isnull().any(axis = 1)]\n",
    "# Drop Missing Indices\n",
    "X_Test_M1 = X_Test_M1.drop(X_Test_M1_Missing_Indices)\n",
    "Y_Test_M1 = Y_Test_M1.drop(X_Test_M1_Missing_Indices)\n",
    "# Reset Index\n",
    "X_Test_M1 = X_Test_M1.reset_index(drop = True)\n",
    "Y_Test_M1 = Y_Test_M1.reset_index(drop = True)\n",
    "print(f'After Drop: {X_Test_M1.shape}')\n",
    "print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lr):\n",
    "\n",
    "    tf.random.set_seed(0) \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    model.add(tf.keras.Input(shape = (6, ), name = 'Input')) # Input Dimension\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units = 1, # Output Dimension\n",
    "        use_bias = True, # Add Bias Parameter (Instead of Concatenating a Column of 1's)\n",
    "        activation = 'sigmoid', # Activation Function\n",
    "        kernel_initializer = tf.keras.initializers.Ones(), # Weight Initializer\n",
    "        bias_initializer = tf.keras.initializers.Ones(), # Bias Initializer\n",
    "        name = 'Output'\n",
    "        ))\n",
    "    \n",
    "    # Compile Model\n",
    "    model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate = lr),\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(lr = 0.001)\n",
    "history = model.fit(x = X_Train_M1, y = Y_Train_M1, validation_data = (X_Validation_M1, Y_Validation_M1), batch_size = 5, epochs = 20, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n",
    "\n",
    "print(f'Weights:\\n {weights}')\n",
    "print(f'Biases: {biases}')\n",
    "\n",
    "Train_Predictions = model.predict(x = X_Train_M1, verbose = 0)\n",
    "Train_Loss, Train_Accuracy = model.evaluate(x = X_Train_M1, y = Y_Train_M1, verbose = 0)\n",
    "print(f'Train Loss: {Train_Loss:.2f} | Train Accuracy: {Train_Accuracy:.2f}')\n",
    "\n",
    "Validation_Predictions = model.predict(x = X_Validation_M1, verbose = 0)\n",
    "Validation_Loss, Validation_Accuracy = model.evaluate(x = X_Validation_M1, y = Y_Validation_M1, verbose = 0)\n",
    "print(f'Validation Loss: {Validation_Loss:.2f} | Validation Accuracy: {Validation_Accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label = 'Train')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.xticks(np.arange(len(history.history['loss'])))\n",
    "plt.title('Loss Over Epochs', fontweight = 'bold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (To-Do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Predictions = model.predict(x = X_Test_M1, verbose = 0)\n",
    "Test_Loss, Test_Accuracy = model.evaluate(x = X_Test_M1, y = Y_Test_M1, verbose = 0)\n",
    "print(f'Test Loss: {Test_Loss:.2f} | Test Accuracy: {Test_Accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model 2**: Height, Weight, Reach + Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_M2 = X_Train.copy()\n",
    "Y_Train_M2 = Y_Train.copy()\n",
    "\n",
    "X_Validation_M2 = X_Validation.copy()\n",
    "Y_Validation_M2 = Y_Validation.copy()\n",
    "\n",
    "X_Test_M2 = X_Test.copy()\n",
    "Y_Test_M2 = Y_Test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Features and Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_M2 = X_Train_M2[['R_Age', 'R_Height', 'R_Weight', 'R_Reach', 'R_SSLPM', 'R_SAC', 'R_SSAPM', 'R_SD', 'R_TAV', 'R_TAC', 'R_TD','R_SAV', 'B_Age', 'B_Height', 'B_Weight', 'B_Reach', 'B_SSLPM', 'B_SAC', 'B_SSAPM', 'B_SD', 'B_TAV', 'B_TAC', 'B_TD', 'B_SAV']]\n",
    "X_Validation_M2 = X_Validation_M2[['R_Age', 'R_Height', 'R_Weight', 'R_Reach', 'R_SSLPM', 'R_SAC', 'R_SSAPM', 'R_SD', 'R_TAV', 'R_TAC', 'R_TD','R_SAV', 'B_Age', 'B_Height', 'B_Weight', 'B_Reach', 'B_SSLPM', 'B_SAC', 'B_SSAPM', 'B_SD', 'B_TAV', 'B_TAC', 'B_TD', 'B_SAV']]\n",
    "X_Test_M2 = X_Test_M2[['R_Age', 'R_Height', 'R_Weight', 'R_Reach', 'R_SSLPM', 'R_SAC', 'R_SSAPM', 'R_SD', 'R_TAV', 'R_TAC', 'R_TD','R_SAV', 'B_Age', 'B_Height', 'B_Weight', 'B_Reach', 'B_SSLPM', 'B_SAC', 'B_SSAPM', 'B_SD', 'B_TAV', 'B_TAC', 'B_TD', 'B_SAV']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "print('TRAIN')\n",
    "print(f'Before Drop: {X_Train_M2.shape}')\n",
    "# Identify Missing Indices\n",
    "X_Train_M2_Missing_Indices = X_Train_M2.index[X_Train_M2.isnull().any(axis = 1)]\n",
    "# Drop Missing Indices\n",
    "X_Train_M2 = X_Train_M2.drop(X_Train_M2_Missing_Indices)\n",
    "Y_Train_M2 = Y_Train_M2.drop(X_Train_M2_Missing_Indices)\n",
    "# Reset Index\n",
    "X_Train_M2 = X_Train_M2.reset_index(drop = True)\n",
    "Y_Train_M2 = Y_Train_M2.reset_index(drop = True)\n",
    "print(f'After Drop: {X_Train_M2.shape}')\n",
    "print('------------------------------------')\n",
    "\n",
    "# VALIDATION\n",
    "print('VALIDATION')\n",
    "print(f'Before Drop: {X_Validation_M2.shape}')\n",
    "# Identify Missing Indices\n",
    "X_Validation_M2_Missing_Indices = X_Validation_M2.index[X_Validation_M2.isnull().any(axis = 1)]\n",
    "# Drop Missing Indices\n",
    "X_Validation_M2 = X_Validation_M2.drop(X_Validation_M2_Missing_Indices)\n",
    "Y_Validation_M2 = Y_Validation_M2.drop(X_Validation_M2_Missing_Indices)\n",
    "# Reset Index\n",
    "X_Validation_M2 = X_Validation_M2.reset_index(drop = True)\n",
    "Y_Validation_M2 = Y_Validation_M2.reset_index(drop = True)\n",
    "print(f'After Drop: {X_Validation_M2.shape}')\n",
    "print('------------------------------------')\n",
    "\n",
    "# TEST\n",
    "print('TEST')\n",
    "print(f'Before Drop: {X_Test_M2.shape}')\n",
    "# Identify Missing Indices\n",
    "X_Test_M2_Missing_Indices = X_Test_M2.index[X_Test_M2.isnull().any(axis = 1)]\n",
    "# Drop Missing Indices\n",
    "X_Test_M2 = X_Test_M2.drop(X_Test_M2_Missing_Indices)\n",
    "Y_Test_M2 = Y_Test_M2.drop(X_Test_M2_Missing_Indices)\n",
    "# Reset Index\n",
    "X_Test_M2 = X_Test_M2.reset_index(drop = True)\n",
    "Y_Test_M2 = Y_Test_M2.reset_index(drop = True)\n",
    "print(f'After Drop: {X_Test_M2.shape}')\n",
    "print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lr):\n",
    "\n",
    "    tf.random.set_seed(0) \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    model.add(tf.keras.Input(shape = (24, ), name = 'Input')) # Input Dimension\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units = 1, # Output Dimension\n",
    "        use_bias = True, # Add Bias Parameter (Instead of Concatenating a Column of 1's)\n",
    "        activation = 'sigmoid', # Activation Function\n",
    "        kernel_initializer = tf.keras.initializers.Ones(), # Weight Initializer\n",
    "        bias_initializer = tf.keras.initializers.Ones(), # Bias Initializer\n",
    "        name = 'Output'\n",
    "        ))\n",
    "    \n",
    "    # Compile Model\n",
    "    model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate = lr),\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(lr = 0.001)\n",
    "history = model.fit(x = X_Train_M2, y = Y_Train_M2, validation_data = (X_Validation_M2, Y_Validation_M2), batch_size = 5, epochs = 20, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n",
    "\n",
    "print(f'Weights:\\n {weights}')\n",
    "print(f'Biases: {biases}')\n",
    "\n",
    "Train_Predictions = model.predict(x = X_Train_M2, verbose = 0)\n",
    "Train_Loss, Train_Accuracy = model.evaluate(x = X_Train_M2, y = Y_Train_M2, verbose = 0)\n",
    "print(f'Train Loss: {Train_Loss:.2f} | Train Accuracy: {Train_Accuracy:.2f}')\n",
    "\n",
    "Validation_Predictions = model.predict(x = X_Validation_M2, verbose = 0)\n",
    "Validation_Loss, Validation_Accuracy = model.evaluate(x = X_Validation_M2, y = Y_Validation_M2, verbose = 0)\n",
    "print(f'Validation Loss: {Validation_Loss:.2f} | Validation Accuracy: {Validation_Accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label = 'Train')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.xticks(np.arange(len(history.history['loss'])))\n",
    "plt.title('Loss Over Epochs', fontweight = 'bold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (To-Do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Predictions = model.predict(x = X_Test_M2, verbose = 0)\n",
    "Test_Loss, Test_Accuracy = model.evaluate(x = X_Test_M2, y = Y_Test_M2, verbose = 0)\n",
    "print(f'Test Loss: {Test_Loss:.2f} | Test Accuracy: {Test_Accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model 3**: Height, Weight, Reach + Attributes + Names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
